{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Neural-Networks\" data-toc-modified-id=\"Introduction-to-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Non-Linear-Regions\" data-toc-modified-id=\"Non-Linear-Regions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Non-Linear Regions</a></span></li><li><span><a href=\"#Error-Functions\" data-toc-modified-id=\"Error-Functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Error Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Proerties-an-error-function-shoud-have\" data-toc-modified-id=\"Proerties-an-error-function-shoud-have-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Proerties an error function shoud have</a></span></li></ul></li><li><span><a href=\"#Move-from-Discrete-to-Continous-Predictions\" data-toc-modified-id=\"Move-from-Discrete-to-Continous-Predictions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Move from Discrete to Continous Predictions</a></span></li><li><span><a href=\"#Multi-Class-Classification-and-Softmax\" data-toc-modified-id=\"Multi-Class-Classification-and-Softmax-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Multi-Class Classification and Softmax</a></span></li><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>One-Hot Encoding</a></span></li><li><span><a href=\"#Maximum-Likelihood-&amp;-Cross-Entropy\" data-toc-modified-id=\"Maximum-Likelihood-&amp;-Cross-Entropy-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Maximum Likelihood &amp; Cross Entropy</a></span></li><li><span><a href=\"#Multi-class-Cross-Entropy\" data-toc-modified-id=\"Multi-class-Cross-Entropy-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Multi-class Cross-Entropy</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Gradient-Descent-(in-Logistic-Regression)\" data-toc-modified-id=\"Gradient-Descent-(in-Logistic-Regression)-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Gradient Descent (in Logistic Regression)</a></span></li><li><span><a href=\"#Implementing-Gradient-Descent\" data-toc-modified-id=\"Implementing-Gradient-Descent-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Implementing Gradient Descent</a></span></li><li><span><a href=\"#Perceptron-vs-Gradient-Descent\" data-toc-modified-id=\"Perceptron-vs-Gradient-Descent-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Perceptron vs Gradient Descent</a></span></li><li><span><a href=\"#Continuous-Perceptron\" data-toc-modified-id=\"Continuous-Perceptron-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Continuous Perceptron</a></span></li><li><span><a href=\"#Non-linear-Data-&amp;-Non-linear-model\" data-toc-modified-id=\"Non-linear-Data-&amp;-Non-linear-model-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Non-linear Data &amp; Non-linear model</a></span></li><li><span><a href=\"#Neural-Network-Architecture\" data-toc-modified-id=\"Neural-Network-Architecture-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Neural Network Architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combine-two-perceptrons-into-a-third\" data-toc-modified-id=\"Combine-two-perceptrons-into-a-third-1.14.1\"><span class=\"toc-item-num\">1.14.1&nbsp;&nbsp;</span>Combine two perceptrons into a third</a></span></li><li><span><a href=\"#Multiple-layers\" data-toc-modified-id=\"Multiple-layers-1.14.2\"><span class=\"toc-item-num\">1.14.2&nbsp;&nbsp;</span>Multiple layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multi-class-classification\" data-toc-modified-id=\"Multi-class-classification-1.14.2.1\"><span class=\"toc-item-num\">1.14.2.1&nbsp;&nbsp;</span>Multi-class classification</a></span></li></ul></li></ul></li><li><span><a href=\"#Feedforward\" data-toc-modified-id=\"Feedforward-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;</span>Feedforward</a></span></li><li><span><a href=\"#Error-function-for-a-neural-network\" data-toc-modified-id=\"Error-function-for-a-neural-network-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;</span>Error function for a neural network</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;</span>Backpropagation</a></span></li></ul></li><li><span><a href=\"#Implementing-the-gradient-descent\" data-toc-modified-id=\"Implementing-the-gradient-descent-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementing the gradient descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-term-in-MSE\" data-toc-modified-id=\"Error-term-in-MSE-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Error term in MSE</a></span></li><li><span><a href=\"#Error-term-in-Log-loss\" data-toc-modified-id=\"Error-term-in-Log-loss-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Error term in Log-loss</a></span></li><li><span><a href=\"#Multiple-layer-perceptrons\" data-toc-modified-id=\"Multiple-layer-perceptrons-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Multiple-layer perceptrons</a></span></li><li><span><a href=\"#Backpropagation-with-MSE-and-Log-loss-error-functions\" data-toc-modified-id=\"Backpropagation-with-MSE-and-Log-loss-error-functions-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Backpropagation with MSE and Log-loss error functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#MSE\" data-toc-modified-id=\"MSE-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>MSE</a></span></li><li><span><a href=\"#Vanishing-gradient\" data-toc-modified-id=\"Vanishing-gradient-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Vanishing gradient</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Regions\n",
    "\n",
    "**Example: Acceptance at a University**\n",
    "> Perceptron algorithm only build a linear boundary but in reality it is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Functions\n",
    "\n",
    "**Pick a direction in which the error (height) descent the most.**\n",
    "\n",
    ">*Note that it is possible to stop at a local minimum*\n",
    "\n",
    "**Discrete vs Continous Error functions?**\n",
    "> Continous Error Function!!\n",
    "\n",
    ">*Discrete error function* may confuse the computer by giving the same error score for all directions.\n",
    "\n",
    "### Proerties an error function shoud have \n",
    "\n",
    "1. Continous\n",
    "2. Give penalty on the misclassified points\n",
    "3. Error decreases as the misclassified points are classified correctly\n",
    "4. Differentiable for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move from Discrete to Continous Predictions\n",
    "\n",
    "*Yes/No* prediction to probability\n",
    "\n",
    "Map the **step function** (discrete) to **Sigmoid function** (continous)\n",
    "\n",
    "**Sigmoid function**\n",
    "\n",
    "- For large positive numbers, it gives value close to 1.\n",
    "\n",
    "- For large negative numbers, it gives value close to 0.\n",
    "\n",
    "- For number close to 0, it gives value close to 0.5.\n",
    "\n",
    "$$\n",
    "\\sigma(Wx+b) = \\frac{1}{1+e^{-Wx+b}},\n",
    "$$\n",
    "where \n",
    "$Wx_b$ is the linear boundary defined in perceptron algorithm.\n",
    "\n",
    "Thus, in the current perceptron algorithm, the activation function changes from step function to sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification and Softmax\n",
    "\n",
    "The **Softmax** function is the equivalent of the sigmoid activation funciton when the problem has 3 or more classes.\n",
    "\n",
    "**Properties of activation function for multi-class classification**\n",
    "- Add to 1;\n",
    "\n",
    "- positive scores;\n",
    "\n",
    "- monotonicity consistent with the linear scores\n",
    "\n",
    "> - **normalize by sum of the scores?**\n",
    "Issues: negative numbers? what if the sum is 0?\n",
    "  \n",
    "> - **return every score to a positive number and then normalize them?**\n",
    "exponential function and do normalization;\n",
    "Also exponetial function is increasing function as score increases\n",
    "\n",
    "$$\n",
    "P(class i) = \\frac{e^{Z_i}}{e^{Z_1} + e^{Z_2} + \\cdots + e^{Z_n}}\n",
    "$$\n",
    "\n",
    "Actually this is the same as sigmoid function when $n = 2$.\n",
    ">Proof:\n",
    "> Sigmoid function gives : \n",
    "$$\n",
    "y = Wx + b; p_1 = \\frac{1}{1 + e^{-y}}, p_0 = \\frac{1}{1+e^{y}}\n",
    "$$\n",
    "\n",
    "> Softmax function gives:\n",
    "$$\n",
    "p_1 = \\frac{1}{1 + e^{z1-z2}}, p_0 = \\frac{1}{1+ e^{z_2-z1}}\n",
    "$$\n",
    "Thus, let $z_2 - z_1= Wx +b$ then two functions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Given data of animals: duck, dog, cat.\n",
    "If we encoding by giving values 0, 1 and 2 for these animals, it introduces the dependency between animal types, which does not exist in our data.\n",
    "\n",
    "One-Hot Encoding will help with this situation.\n",
    "\n",
    "|Animal|Duck?|Dog?|Cat?|\n",
    "|:-|:-|:-|:-|\n",
    "|Duck| 1 | 0 | 0|\n",
    "|Dog | 0| 1| 0|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood & Cross Entropy\n",
    "\n",
    "**Maximizing the probability of observing the data given the model.**\n",
    "\n",
    "- Likelihood: \n",
    "$$\n",
    "\\prod{probability}\n",
    "$$\n",
    "- Cross Entropy\n",
    "$$\n",
    "-\\sum_{points}\\ln(probability)\n",
    "$$\n",
    "\n",
    "**High likelihood ~ Low cross entropy**\n",
    "\n",
    "*Actually, each $-ln(probability)$ gives a measurement of the error.*\n",
    "\n",
    "$$\n",
    "CrossEntropy = -\\sum_{i = 1}^n y_i \\ln (p_i) + (1-y_i)* \\ln(1-p_i)\n",
    "$$\n",
    "where $y_i = 1 \\text{ or } 0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Cross-Entropy\n",
    "\n",
    "$$\n",
    "CrossEntropy = - \\sum_{i = 1}^n \\sum_{j = 1}^m y_{i,j} \\ln (p_{i,j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "It is the building block of all that constitues Deep Learning.\n",
    "\n",
    "\n",
    "$$\n",
    "Error function = - \\frac{1}{n} \\sum_{i = 1}^n (1-y_i)\\ln(1-\\hat{y}_i) + y_i * \\ln(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "or\n",
    "$$\n",
    "E(W, b) =  - \\frac{1}{n} \\sum_{i = 1}^n (1-y_i)\\ln(1-\\sigma(Wx_i + b) + y_i * \\ln(\\sigma(Wx_i + b))\n",
    "$$\n",
    "\n",
    "In multi-class:\n",
    "\n",
    "$$\n",
    "Error function = - \\frac{1}{n} \\sum_{ i = 1}^n \\sum{j = 1}^m y_{i,j} \\ln(p_{i,j})\n",
    "$$\n",
    "\n",
    "**minimizing the error function by gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent (in Logistic Regression)\n",
    "\n",
    "Take the negative derivative of the error function\n",
    "\n",
    "For error function of one point with coordinates $(x_1, x_2, \\cdots, x_n)$ and prediciton $\\hat{y}$.\n",
    "\n",
    "\n",
    "- $\\hat{y} = \\sigma(Wx + b)$\n",
    "\n",
    "- $\\nabla E = -(y - \\hat{y}) (x_1, x_2, \\cdots, x_n, 1)$, where $x_1, x_2, \\cdots, x_n$ are the coordinates.\n",
    "\n",
    "- $\\alpha = 0.1$\n",
    "\n",
    "- $w_i' = w_i - \\alpha * \\frac{\\partial E}{\\partial w_i} =w_i - \\alpha * ( \\hat{y} - y))x_i$\n",
    "\n",
    "- $b' = b - \\alpha * \\frac{\\partial E}{\\partial b} = b - \\alpha * (\\hat{y} - y)$\n",
    "\n",
    "- $\\hat{y} = \\sigma(W'x + b')$\n",
    "\n",
    "Note that the gradient should be divided by $n$, then the learning rate is $\\frac{\\alpha}{n}$. We abuse the notation by $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "\n",
    "> Please refer to the lab in the folder *'GradientDescent'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "|  |GradientDescent|Perceptron|\n",
    "|:-|:-|:-|\n",
    "|Update reference points| All | Misclassified|\n",
    "|Weight update for each point|$w_j + \\alpha (y-\\hat{y})x_j$ | $w_j \\pm \\alpha x_j$; $+$ if positive|\n",
    "|Correctly classiefied| go farther away| do nothing|\n",
    "|Misclassifed| come closer| come closer|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Perceptron\n",
    "\n",
    "Return probability...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Data & Non-linear model\n",
    "\n",
    "Neural network is useful when the boundary is non-linear\n",
    "\n",
    "Idea: create probability separating regions to two with non-linear boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "### Combine two perceptrons into a third\n",
    "\n",
    "- In perceptron1: $p_1 = 0.7, w_1 = 7$\n",
    "- In perceptron2: $p_2 = 0.8, w_2 = 5$\n",
    "- bias: -6\n",
    "- In combined perceptron: $\\sigma(p_1*w_1 + p_2*w_2 - 6)$\n",
    "\n",
    "**Include Figure**\n",
    "\n",
    ">Define the combination of two new perceptrons as $w_1 *0.4 + w_2 * 0.6  + b$. Which of the following values would return the final probability to be 0.88?\n",
    "$$w_1: 2, w_2: 6, b : -2$$\n",
    "$$w_!:3, w_2:5, b: -2.2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(w1, w2, b):\n",
    "    x = w1*0.4 + w2*0.6 +b\n",
    "    return 1/(1+np.exp(-x))\n",
    "sigmoid(3,5, -2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple layers\n",
    "\n",
    "Simple layer neural network structure:\n",
    "\n",
    "- Input: Input layer , number of nodes indicate the input dimension\n",
    "- Linear models: Hidden layer\n",
    "- Output (Non-linear): output layer, number of nodes indicate the number of classes to classify\n",
    "\n",
    "It can be more complicated by \n",
    "1. Add more nodes to the input, hidden, and output layers\n",
    "2. Add more layers\n",
    "\n",
    "#### Multi-class classification\n",
    "\n",
    "**Include figure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "It is the process neural networks use to turn the input into an output.\n",
    "\n",
    "It take input values combined with linear combination and sigmoid function.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma W^{(2)} \\times \\sigma \\times W^{(1)}x\n",
    "$$\n",
    "\n",
    "**Includefigures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function for a neural network\n",
    "\n",
    "It is the same as the error function for simple layer perceptron defined before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- Feedforward operation\n",
    "- Compare the output of themodel with the desired output\n",
    "- Calculating the error\n",
    "- Running the feedforward backwards to spread the error to each of the weights (Check the error: check which linear models in the hidden layer does better, reduce the weight from the bad model and increase the weight from the better model)\n",
    "- use it to update the weights and get a better model \n",
    "- Continue it until we have a model that is good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error term with MSE function and Log-loss function would be different.\n",
    "\n",
    "Given $\\hat{y} = f(h), h = w_1x_1+ w_2x_2 + \\cdots +b$, where $f(h)$ is called the activation function as before.\n",
    "\n",
    "## Error term in MSE\n",
    "\n",
    "$$\n",
    "Error = \\frac{1}{2n}\\sum_{i = 1}^n(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "The gradient for $w_j$ is given by \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = -(y -\\hat{y})f'(h)x_j\n",
    "$$\n",
    "\n",
    "**Error Term**\n",
    "\n",
    "$$\n",
    "\\delta = (y-\\hat{y})f'(h)\n",
    "$$\n",
    "\n",
    "**Weight Update**\n",
    "\n",
    "$$\n",
    "w_i = w_i + \\eta \\delta x_i/n\n",
    "$$\n",
    "\n",
    "**Multiple Output unit**\n",
    "\n",
    "$$\n",
    "\\delta_j = (y_j - \\hat{y}_j)f'(h_j)\n",
    "$$\n",
    "\n",
    "## Error term in Log-loss\n",
    "\n",
    "$$\n",
    "Error = -\\frac{1}{n}\\sum_{i = 1}^n y_i \\ln(\\hat{y}_i) + (1-y_i) \\ln(1- \\hat{y}_i)\n",
    "$$\n",
    "The gradient for $w_j$ is given by \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = -(y -\\hat{y})x_j\n",
    "$$\n",
    "\n",
    "**Error Term**\n",
    "\n",
    "$$\n",
    "\\delta = (y-\\hat{y})\n",
    "$$\n",
    "\n",
    "**Weight Update**\n",
    "\n",
    "$$\n",
    "w_i = w_i + \\eta \\delta x_i/n\n",
    "$$\n",
    "\n",
    "**Multiple Output unit**\n",
    "\n",
    "$$\n",
    "\\delta_j = (y_j - \\hat{y}_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-layer perceptrons\n",
    "\n",
    "`arr[:, None]` creates a column vector.\n",
    "\n",
    "$$h = [x1, x2, 1]W$$\n",
    "\n",
    "W is given by \n",
    "\n",
    "| | hidden unit 1| hidden unit 2| \n",
    "|:-|:-|:-|\n",
    "|input unit 1($x_1$)|$W_{11}$ | $W_{12}$ |\n",
    "|input unit 2 | $W_{2,1}$ |$W_{2,2}$|\n",
    "|input unit 3| $W_{3,1}$ | $w_{3,2}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with MSE and Log-loss error functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "\n",
    "Let $i$ denote the input index, $j$ denote the hidden index, $k$ denote the output index.\n",
    "\n",
    "- The error attributed to output unit $k$:\n",
    "$$\n",
    "\\delta_k^o = (y_k - \\hat{y}_k) f'(H_k)\n",
    "$$\n",
    "\n",
    "- The error attributed to hidden unit $j$:\n",
    "$$\n",
    "\\delta_j^h = \\sum_{k} W_{j,k}\\delta_k^o f'(h_j)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\delta^h = np.dot(W^{(hidden-output)}, \\delta^o) * f'(h)\n",
    "$$\n",
    "\n",
    "- The gradient descent step for input unit $i$:\n",
    "$$\n",
    "\\Delta w_{i,j} = \\eta \\delta_j^h x_i\n",
    "$$\n",
    "\n",
    "- The gradient descent step for the hidden unit $j$:\n",
    "$$\n",
    "\\Delta w_{j,k } = \\eta \\delta_k^o f(h_j)\n",
    "$$\n",
    "\n",
    "\n",
    "In general, it follows the rule:\n",
    "$$\n",
    "\\Delta w_{p,q} = \\eta \\delta_{output} V_{in}\n",
    "$$\n",
    "where $\\delta_{output}$ is the output error, $V_{in}$ is the input to the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradient\n",
    "\n",
    "When the activation funtion is sigmoid function, the weight steps are quickly reduced to tiny values in layear near the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 12])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([2,3])\n",
    "b = np.array([3,4])\n",
    "a*b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "25px",
    "width": "164px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
